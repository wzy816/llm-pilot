{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356c7f65",
   "metadata": {},
   "source": [
    "##  n text on n image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"/mnt/huggingface/hub/models--google--owlvit-base-patch32\", local_files_only=True)\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"/mnt/huggingface/hub/models--google--owlvit-base-patch32\", local_files_only=True)\n",
    "\n",
    "\n",
    "# load images\n",
    "import glob\n",
    "images = []\n",
    "original_dir = '/mnt/owlvit/test_img'\n",
    "\n",
    "for image_fp in glob.glob(f'{original_dir}/*.jpg'):\n",
    "    images.append(Image.open(image_fp))\n",
    "\n",
    "\n",
    "# prepare texts\n",
    "texts = [[\"a photo of a human face\", \n",
    "          \"a photo of a people face\", \n",
    "          \"a photo of a vehicle license plate\", \n",
    "          \"a photo of a car plate\"] for i in range(len(images))]\n",
    "\n",
    "inputs = processor(text=texts, images=images, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# inference\n",
    "with torch.inference_mode(), torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.Tensor([x.size[::-1] for x in images])\n",
    "    results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\n",
    "    \n",
    "# plot results  \n",
    "from PIL import ImageDraw,ImageFont\n",
    "for text, image, result in zip(texts, images, results):\n",
    "    boxes, scores, labels = result[\"boxes\"], result[\"scores\"], result[\"labels\"]\n",
    "    im = image.copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        font = ImageFont.truetype(\"Overpass-Bold.ttf\", 40)\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=10)\n",
    "        t = text[label].replace(\"a photo of a \",\"\")\n",
    "        draw.text((xmin, ymin-40), f\"{t}: {round(score.item(), 3)}\", fill=\"white\", font=font)    \n",
    "\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caabadf",
   "metadata": {},
   "source": [
    "## 1 query image on 1 image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec01a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"/mnt/huggingface/hub/models--google--owlvit-base-patch32\", local_files_only=True)\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"/mnt/huggingface/hub/models--google--owlvit-base-patch32\", local_files_only=True)\n",
    "\n",
    "image = Image.open(\"/mnt/owlvit/test_img/1701655590224868.jpg\")\n",
    "query_image = Image.open(\"/mnt/owlvit/symbol/van1.jpg\")\n",
    "\n",
    "inputs = processor(images=image, query_images=query_image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.image_guided_detection(**inputs)\n",
    "    target_sizes = torch.Tensor([image.size[::-1]])\n",
    "    results = processor.post_process_image_guided_detection(\n",
    "        outputs=outputs, threshold=0.6, nms_threshold=0.3, target_sizes=target_sizes\n",
    "    )\n",
    "\n",
    "boxes, scores = results[0][\"boxes\"], results[0][\"scores\"]\n",
    "\n",
    "from PIL import ImageDraw,ImageFont\n",
    "\n",
    "im = image.copy()\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for box, score in zip(boxes, scores):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    font = ImageFont.truetype(\"Overpass-Bold.ttf\", 40)\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=10)\n",
    "    draw.text((xmin, ymin-40), f\"{round(score.item(), 3)}\", fill=\"white\", font=font)    \n",
    "\n",
    "display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212c089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
